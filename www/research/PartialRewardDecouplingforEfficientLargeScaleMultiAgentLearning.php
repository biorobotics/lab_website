<?php include $_SERVER['DOCUMENT_ROOT'].'/phpincludes/header.php'; ?>

<!-- Basic Page Needs - Edit keywords and page title here
============================= -->
	<title>Partial Reward Decoupling for Efficient Large-Scale Multi-Agent Learning - Biorobotics</title>
	<meta name="Keywords" content="Carnegie Mellon University, robotics, Multi-Agent Planning">
	<meta name="description" content ="Partial Reward Decoupling for Efficient Large-Scale Multi-Agent Learning - Carnegie Mellon Biorobotics" /><!-- Banner and Menu - Do not change
============================= -->
<?php include $_SERVER['DOCUMENT_ROOT']."/phpincludes/banner.php";?>

<!-- Content - Add content for your page here
============================= -->
    
	<div class="row"><h3 class="c20" id="h.urb7mha8l3su"><b><span class="c15">Multi-Agent Planning</b> - Partial Reward Decoupling for Efficient Large-Scale Multi-Agent Learning</span></h3><p class="c2"><span class="c5"></span></p><p class="c10"><span class="c11">One of the preeminent obstacles to scaling multi-agent reinforcement learning to large numbers of agents &nbsp;is assigning credit to individual agents&#39; actions. &nbsp;This project seeks to address this credit assignment problem with an approach that we call </span><span class="c11 c32">partial reward decoupling</span><span class="c11">&nbsp;(PRD). PRD attempts to decompose large cooperative multi-agent RL problems into decoupled subproblems involving subsets of agents, thereby simplifying credit assignment. &nbsp;Our initial work has empirically demonstrate that decomposing the RL problem using PRD in both actor-critic and proximal policy optimization algorithms results in lower variance policy gradient estimates, which improves data efficiency, learning stability, and asymptotic performance across a wide array of multi-agent RL tasks, compared related approaches that do not use PRD, such as counterfactual multi-agent policy gradient (COMA), a state-of-the-art MARL algorithm.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 240.00px; height: 240.00px;"><img alt="" src="images/PartialRewardDecouplingforEfficientLargeScaleMultiAgentLearning/image82.png" style="width: 240.00px; height: 240.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 240.00px; height: 240.00px;"><img alt="" src="images/PartialRewardDecouplingforEfficientLargeScaleMultiAgentLearning/image5.png" style="width: 240.00px; height: 240.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10 c51"><span class="c16 c11"></span></p><p class="c10"><span class="c11">[1] Freed, Benjamin, et al. &quot;Learning Cooperative Multi-Agent Policies With Partial Reward Decoupling.&quot; IEEE Robotics and Automation Letters 7.2 (2021): 890-897. </span><span class="c29 c13"><a class="c24" href="https://www.google.com/url?q=https://ieeexplore.ieee.org/abstract/document/9653841?casa_token%3D938Xzf30YigAAAAA:vWmsqEbPmBwpxf-Z1WSp5RVodX6N37ye3OPPqMvBpvSuZ9c6rRTiYV7_l00RlriAhv72itZfMw&amp;sa=D&amp;source=editors&amp;ust=1672867222599058&amp;usg=AOvVaw2eV_f14qF-T5KW879asdQF">https://ieeexplore.ieee.org/abstract/document/9653841?casa_token=938Xzf30YigAAAAA:vWmsqEbPmBwpxf-Z1WSp5RVodX6N37ye3OPPqMvBpvSuZ9c6rRTiYV7_l00RlriAhv72itZfMw</a></span></p><p class="c10 c51"><span class="c16 c11"></span></p><hr style="page-break-before:always;display:none;"><p class="c10 c51"><span class="c16 c11"></span></p><p class="c10"><span class="c29"><a class="c24" href="#h.jpmbcvqkcs3b">[Home]</a></span></p><!-- <div class="six columns">
			<h3><b>Modularity</b> - Modular manipulators</h3>
			<p>
				<b>Modular robotic systems</b> have the potential to be adapted to varying tasks using a single set of reconfigurable hardware, enabling customizable robots to be developed faster and more economically than conventional robots. Currently, even with the engineering experience, calculations and intuitions, it is not immediately evident how to construct and program a modular robot for a task. We research methods to automatically synthesize the design and controllers for modular robot arms.
			</p>
			<div class="eleven columns u-pull-left">
				<img src="images/Modularity/fig2.png" alt="ModularityF2" width="100%;" style="padding-left:1rem;">
				<p class="caption"> Figure2 : A prototype of a robot arm made from HEBI modular actuators.
				</p>
		</div>
		</div>
		<div class="five columns u-pull-right">
			<img src="images/Modularity/fig1.png" alt="ModularityF1" width="100%;" style="padding-left:1rem;">
			<p class="caption"> Figure 1:  Supernumerary arm implemented in hardware, reaching  one  of  the  targets  on  the  overhead  workpiece.  The  end
			effector  is  a  flat  rubberized  plate  with  spring-loaded  hinge.
			The  arm,  attached  to  a  backpack  control  and  power  pack,
			lends the user a helping hand
			</p>
		</div> -->
	</div>
<!-- Gallery - include if page has gallery elements
============================= -->
<?php include $_SERVER['DOCUMENT_ROOT']."/phpincludes/gallery.php";?>

<!-- Footer - Do not change
============================= -->
<?php include $_SERVER['DOCUMENT_ROOT']."/phpincludes/footer.php";?>